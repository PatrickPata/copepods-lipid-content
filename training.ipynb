{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a cleanest notebook, more for archiving purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from utils import (\n",
    "    generic_image_path,\n",
    "    generic_image_path_processed,\n",
    "    generic_segmentation_path,\n",
    "    generic_segmentation_path_processed,\n",
    "    paste_imgs,\n",
    ")\n",
    "np.int = np.int32 # Need to add this to make preset model work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask(source: Image.Image, mask: Image.Image) -> Image.Image:\n",
    "    source = source.convert('RGBA')\n",
    "    # mask = Image.fromarray(np.r_[mask] * 255).convert('RGBA')\n",
    "    mask = mask.convert('RGBA')\n",
    "    M = np.r_[mask]\n",
    "    M[:, :, 1:2] = 0\n",
    "    M[:, :, 3] = 120\n",
    "    \n",
    "    mask = Image.fromarray(M)\n",
    "    return Image.alpha_composite(source, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to not keep changing the folder names in the utils file. I organized the \"data/\" folder to be inside different resolution scenario folders.\n",
    "# So revise the root path assigned when necessary.\n",
    "root = Path(\"../downscaled_2/\")\n",
    "# root = Path(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like image_path and segmentation_path folders are not used. Maybe holds original files?\n",
    "# These functions in utils set the folder names there. Must change the root folder?\n",
    "\n",
    "# image_path = generic_image_path(root)\n",
    "image_path_processed = generic_image_path_processed(root)\n",
    "# segmentation_path = generic_segmentation_path(root)\n",
    "segmentation_path_processed = generic_segmentation_path_processed(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(p:Path):\n",
    "    return segmentation_path_processed(p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segmentation_file_processed'] = df.filename.apply(segmentation_path_processed)\n",
    "df['image_file_processed'] = df.filename.apply(image_path_processed)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataBlock(\n",
    "    blocks=(ImageBlock, MaskBlock(codes=[\"nothing\", \"lipid_sac\"])),\n",
    "    get_x=ColReader(\"image_file_processed\"), # alternatively can be get_items? \n",
    "    # get_x is used because something was applied to the input data (transformations).\n",
    "    get_y=ColReader(\"segmentation_file_processed\"),\n",
    "    splitter=ColSplitter(\"is_valid\"),\n",
    "    #splitter=RandomSplitter(valid_pct=0.8, seed=42),\n",
    "    \n",
    "     # These settings augments the input images to include padding, crop/zoom, and rotations.\n",
    "    batch_tfms=[Normalize.from_stats(*imagenet_stats), *aug_transforms(pad_mode='zeros', max_rotate=180, max_zoom = 2)]\n",
    ").dataloaders(df, batch_size=1,\n",
    "             drop_last=True) #changed batch_size from 4 to 1\n",
    "# Added drop_last=True to potentially fix the Error: Expected more than 1 value per channel error\n",
    "# A higher batch size will result in a RuntimeError: stack expects each tensor to be equal size ... because the image batches dont have the same sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.one_batch()\n",
    "# dls.show_batch()\n",
    "\n",
    "xb, yb = dls.one_batch()\n",
    "\n",
    "print(xb.shape)\n",
    "print(yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss:\n",
    "    \"Dice and Focal combined\"\n",
    "    def __init__(self, axis=1, smooth=1., alpha=1.):\n",
    "        store_attr()\n",
    "        self.focal_loss = FocalLossFlat(axis=axis)\n",
    "        self.dice_loss =  DiceLoss(axis, smooth)\n",
    "        \n",
    "    def __call__(self, pred, targ):\n",
    "        return self.focal_loss(pred, targ) + self.alpha * self.dice_loss(pred, targ)\n",
    "    \n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(preds:Tensor, targs:Tensor, eps:float=1e-8):\n",
    "    \"\"\"Computes the Jaccard loss, a.k.a the IoU loss.\n",
    "    Notes: [Batch size,Num classes,Height,Width]\n",
    "    Args:\n",
    "        targs: a tensor of shape [B, H, W] or [B, 1, H, W].\n",
    "        preds: a tensor of shape [B, C, H, W]. Corresponds to\n",
    "            the raw output or logits of the model. (prediction)\n",
    "        eps: added to the denominator for numerical stability.\n",
    "    Returns:\n",
    "        iou: the average class intersection over union value \n",
    "             for multi-class image segmentation\n",
    "    \"\"\"\n",
    "    num_classes = preds.shape[1]\n",
    "    \n",
    "    # Single class segmentation?\n",
    "    if num_classes == 1:\n",
    "        true_1_hot = torch.eye(num_classes + 1)[targs.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
    "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
    "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "        pos_prob = torch.sigmoid(preds)\n",
    "        neg_prob = 1 - pos_prob\n",
    "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
    "        \n",
    "    # Multi-class segmentation\n",
    "    else:\n",
    "        # Convert target to one-hot encoding\n",
    "        # true_1_hot = torch.eye(num_classes)[torch.squeeze(targs,1)]\n",
    "        true_1_hot = torch.eye(num_classes)[targs.squeeze(1)]\n",
    "        \n",
    "        # Permute [B,H,W,C] to [B,C,H,W]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Take softmax along class dimension; all class probs add to 1 (per pixel)\n",
    "        probas = F.softmax(preds, dim=1)\n",
    "        \n",
    "    true_1_hot = true_1_hot.type(preds.type())\n",
    "    \n",
    "    # Sum probabilities by class and across batch images\n",
    "    dims = (0,) + tuple(range(2, targs.ndimension()))\n",
    "    intersection = torch.sum(probas * true_1_hot, dims) # [class0,class1,class2,...]\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)  # [class0,class1,class2,...]\n",
    "    union = cardinality - intersection\n",
    "    iou = (intersection / (union + eps)).mean()   # find mean of class IoU values\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loss = CombinedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(dls, resnet34, loss_func=combined_loss, metrics=[IoU])\n",
    "\n",
    "# learn.summary()\n",
    "# To identify the optimal learning rate if necessary:\n",
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a patch for downscale/2 and /4 to resolve the batchnorm issue when training with the downscaled data because it reduces the input to a single dimension.\n",
    "\n",
    "# Get the model from the learner\n",
    "model = learn.model\n",
    "\n",
    "# Function to modify BatchNorm layers\n",
    "def modify_batchnorm(module):\n",
    "    for child_name, child in module.named_children():\n",
    "        if isinstance(child, nn.BatchNorm2d):\n",
    "            # Replace BatchNorm2d with a conditional version or skip logic\n",
    "            setattr(module, child_name, ConditionalBatchNorm(child.num_features))\n",
    "        else:\n",
    "            modify_batchnorm(child)\n",
    "\n",
    "# Define a custom conditional batch normalization layer\n",
    "class ConditionalBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ConditionalBatchNorm, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply BatchNorm only if the spatial dimensions are greater than 1\n",
    "        if x.size(2) > 1 and x.size(3) > 1:\n",
    "            return self.bn(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Apply the function to modify batch normalization layers in the model\n",
    "modify_batchnorm(model)\n",
    "\n",
    "# Update the learner's model with the modified version\n",
    "learn.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbs are callbacks used for checkpointing\n",
    "cbs = []\n",
    "cbs.append(EarlyStoppingCallback(patience=5))\n",
    "cbs.append(SaveModelCallback(fname=\"model_resnet34_1015\")) # same name for both /2 and /4\n",
    "cbs.append(GradientAccumulation(n_acc=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = unet_learner(dls, resnet34)\n",
    "\n",
    "# fine_tune attempmts to improve the model performance. Here the learning rate can be set which could be determined\n",
    "# from learn.lr_find(). The number inside lr_find() is the number of epochs allowed.\n",
    "learn.fine_tune(40, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For exploring the model results\n",
    "learn.show_results()\n",
    "\n",
    "# # Errors sorted\n",
    "# interp = SegmentationInterpretation.from_learner(learn) # Also has a softmax error\n",
    "# interp.plot_top_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(\"models/learner_down2_1015.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"model_backup_down2_1015\") # Should this be the same file name as the one in SaveModelCallback()?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn2 = load_learner(\"models/learner_down4_1015.pkl\", cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn2.load(\"model_resnet34_1015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls2 = DataBlock(\n",
    "    blocks=(ImageBlock, MaskBlock(codes=[\"nothing\", \"lipid_sac\"])),\n",
    "    get_x=ColReader(\"image_file_processed\"),\n",
    "    get_y=ColReader(\"segmentation_file_processed\"),\n",
    "    splitter=ColSplitter(\"is_valid\"),\n",
    "    batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    ").dataloaders(df, batch_size=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = learn2.get_preds(dl=dls2[1], reorder=False, with_input=True)\n",
    "# Current error is: 'list' object has no attribute 'softmax'. This error supposedly arises when the output of the learner is a list rather than a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b87976caf3c7106082cf1714fc5ebab64a09dc193e0bee06d509a95d77726b49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
